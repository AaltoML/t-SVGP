{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a36b7da",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "# MNIST Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3012605",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import gpflow\n",
    "from gpflow.optimizers import NaturalGradient\n",
    "from src.models.tsvgp import t_SVGP\n",
    "import logging\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "tf.get_logger().setLevel(logging.ERROR)\n",
    "\n",
    "rng = np.random.RandomState(1)\n",
    "tf.random.set_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fbca804",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "## Loading MNIST data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b338084",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_mnist():\n",
    "    mnist_train, mnist_test = tf.keras.datasets.mnist.load_data()\n",
    "    \n",
    "    x, y = mnist_train\n",
    "    x = tf.reshape(x, [x.shape[0], -1]).numpy()\n",
    "    x = x.astype(np.float64)/255\n",
    "    y = np.reshape(y, (-1, 1))\n",
    "    y = np.int64(y)\n",
    "    \n",
    "    xt, yt = mnist_test\n",
    "    xt = tf.reshape(xt, [xt.shape[0], -1]).numpy()\n",
    "    xt = xt.astype(np.float64)/255\n",
    "    yt = np.reshape(yt, (-1, 1))\n",
    "    yt = np.int64(yt)\n",
    "\n",
    "    perm = rng.permutation(x.shape[0])\n",
    "    np.take(x, perm, axis=0, out=x)\n",
    "    np.take(y, perm, axis=0, out=y)\n",
    "\n",
    "    return x, y, xt, yt\n",
    "\n",
    "\n",
    "M = 100         # Number of inducing points\n",
    "C = 10          # Number of classes\n",
    "mb_size = 200   # Size of minibatch during training\n",
    "nit = 150       # Number of training iterations\n",
    "nat_lr = 0.03   # Learning rate for E-step (variational params)\n",
    "adam_lr = 0.02  # Learning rate for M-step (hyperparams)\n",
    "n_e_steps = 1   # Number of E-steps per step\n",
    "n_m_steps = 1   # Number of M-steps per step\n",
    "\n",
    "\n",
    "\n",
    "# Initial hyperparameters\n",
    "ell = 1.0\n",
    "var = 1.0\n",
    "\n",
    "# Load data\n",
    "X, Y, XT, YT = load_mnist()\n",
    "\n",
    "# Training data\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X, Y)).repeat().shuffle(X.shape[0])\n",
    "\n",
    "# Initialize inducing locations to the first M inputs in the data set\n",
    "Z = X[:M, :].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f751b3b",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 2
   },
   "source": [
    "## Declaring Classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b77894",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "models = []\n",
    "names = []\n",
    "\n",
    "# Set up the 'standard' q-SVGP model\n",
    "m = gpflow.models.SVGP(\n",
    "        kernel=gpflow.kernels.Matern52(lengthscales=np.ones((1, X.shape[1]))*ell, variance=var),\n",
    "        likelihood=gpflow.likelihoods.Softmax(C), \n",
    "        inducing_variable=Z.copy(),\n",
    "        num_data=X.shape[0],\n",
    "        whiten=True,\n",
    "        num_latent_gps=C)\n",
    "\n",
    "gpflow.set_trainable(m.q_mu, False)\n",
    "gpflow.set_trainable(m.q_sqrt, False)\n",
    "\n",
    "models.append(m)\n",
    "names.append('q-SVGP')\n",
    "\n",
    "# Set up the t-SVGP model\n",
    "m = t_SVGP(\n",
    "        kernel=gpflow.kernels.Matern52(lengthscales=np.ones((1, X.shape[1]))*ell, variance=var),\n",
    "        likelihood=gpflow.likelihoods.Softmax(C),\n",
    "        inducing_variable=Z.copy(),\n",
    "        num_data=X.shape[0],\n",
    "        num_latent_gps=C)\n",
    "\n",
    "gpflow.set_trainable(m.lambda_1, False)\n",
    "gpflow.set_trainable(m.lambda_2_sqrt, False)\n",
    "\n",
    "models.append(m)\n",
    "names.append('t-SVGP')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c66264",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "## Training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd116e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train(model, iterations):\n",
    "    \"\"\"\n",
    "    Utility function for training SVGP models with natural gradients\n",
    "\n",
    "    :param model: GPflow model\n",
    "    :param iterations: number of iterations\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Optimizing model: \", model.name)\n",
    "\n",
    "    natgrad_opt = NaturalGradient(gamma=nat_lr)\n",
    "\n",
    "    tf.random.set_seed(4)\n",
    "    train_iter = iter(train_dataset.batch(mb_size))\n",
    "\n",
    "    tf.random.set_seed(4)\n",
    "    train_iter2 = iter(train_dataset.batch(mb_size))\n",
    "\n",
    "    training_loss = model.training_loss_closure(train_iter, compile=True)\n",
    "    training_loss2 = model.training_loss_closure(train_iter2, compile=True)\n",
    "\n",
    "    # Define the M-step (that is called in the same way for both)\n",
    "    optimizer = tf.optimizers.Adam(adam_lr)\n",
    "\n",
    "    @tf.function\n",
    "    def optimization_m_step(training_loss, params):\n",
    "        optimizer.minimize(training_loss, var_list=params)\n",
    "\n",
    "    # Define the E-steps\n",
    "    def optimization_step_nat(training_loss, variational_params):\n",
    "        natgrad_opt.minimize(training_loss, var_list=variational_params)  \n",
    "                    \n",
    "    @tf.function\n",
    "    def optimization_e_step(model, data):\n",
    "        model.natgrad_step(data, lr=nat_lr)\n",
    "\n",
    "    for _ in tqdm(range(iterations)):\n",
    "        data = next(train_iter)\n",
    "\n",
    "        if model.name == 'svgp' and model.q_mu.trainable == False:\n",
    "            variational_params = [(model.q_mu, model.q_sqrt)]\n",
    "            optimization_e_step = tf.function(lambda loss: optimization_step_nat(loss, variational_params))\n",
    "    \n",
    "            for i in range(n_e_steps):\n",
    "                optimization_e_step(training_loss)\n",
    "            for j in range(n_m_steps):\n",
    "                optimization_m_step(training_loss2, model.trainable_variables)\n",
    "    \n",
    "        elif model.name == 't_svgp':\n",
    "            for i in range(n_e_steps):\n",
    "                optimization_e_step(model, data)\n",
    "            for i in range(n_m_steps):\n",
    "                optimization_m_step(training_loss2, model.trainable_variables)\n",
    "            \n",
    "        else:\n",
    "            raise(\"No training setup for this model.\")\n",
    "\n",
    "\n",
    "for m, name in zip(models, names):\n",
    "    t0 = time.time()\n",
    "    train(m, nit)\n",
    "    t = time.time()-t0\n",
    "\n",
    "    # Calculate NLPD on test set\n",
    "    nlpd = -tf.reduce_mean(m.predict_log_density((XT, YT))).numpy()\n",
    "    \n",
    "    # Calculate accuracy on test set\n",
    "    pred = m.predict_y(XT)[0]\n",
    "    pred_argmax = tf.reshape(tf.argmax(pred, axis=1), (-1, 1))\n",
    "    acc = np.mean(pred_argmax == YT)    \n",
    "    \n",
    "    print('Training time for', name, 'was', t, 'seconds')\n",
    "    print(name, 'test NLPD =', nlpd)\n",
    "    print(name, 'test accuracy =', acc)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_markers": "\"\"\""
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
