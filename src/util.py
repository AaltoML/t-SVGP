"""
Utilities for the model classes
"""

from typing import Optional

import tensorflow as tf
from gpflow import default_float


def conditional_from_precision_sites_white(
    Kuu: tf.Tensor,
    Kff: tf.Tensor,
    Kuf: tf.Tensor,
    l: tf.Tensor,
    L: tf.Tensor = None,
    L2=None,
    jitter=1e-9,
):
    """
    Given a gâ‚ and g2, and distribution p and q such that
      p(gâ‚‚) = N(gâ‚‚; 0, Kuu)

      p(gâ‚) = N(gâ‚; 0, Kff)
      p(gâ‚ | gâ‚‚) = N(gâ‚; Kfu (Kuuâ»Â¹) gâ‚‚, Kff - Kfu (Kuuâ»Â¹) Kuf)

    And  q(gâ‚‚) = N(gâ‚‚; Î¼, Î£) such that
        Î£â»Â¹  = Kuuâ»Â¹  + Kuuâ»Â¹LLáµ€Kuuâ»Â¹
        Î£â»Â¹Î¼ = Kuuâ»Â¹l

    This method computes the mean and (co)variance of
      q(gâ‚) = âˆ« q(gâ‚‚) p(gâ‚ | gâ‚‚) dgâ‚‚ = N(gâ‚‚; Î¼*, Î£**)

    with
    Î£** = k** - kfu Kuuâ»Â¹ kuf - kfu Kuuâ»Â¹ Î£ Kuuâ»Â¹ kuf
        = k** - kfu Kuuâ»Â¹kuf + kfu (Kuu + LLáµ€)â»Â¹ kuf
    Î¼* = k*u Kuuâ»Â¹ m
       = k*u Kuuâ»Â¹ Î›â»Â¹ Kuuâ»Â¹ l
       = k*u (Kuu + LLáµ€)â»Â¹ l

    Inputs:
    :param Kuu: tensor M x M
    :param Kff: tensor N x 1
    :param Kuf: tensor M x N
    :param L: tensor L x M x M
    :param l: tensor M x L
    """

    shape_constraints = [
        (Kuu, ["M", "M"]),
        (Kuf, ["M", "N"]),
        (Kff, ["N", 1]),
        (l, ["M", "L"]),
    ]

    if L2 is not None:
        shape_constraints.append(
            (L2, ["L", "M", "M"]),
        )
    if L is not None:
        shape_constraints.append(
            (L, ["L", "M", "M"]),
        )
    tf.debugging.assert_shapes(
        shape_constraints,
        message="conditional_from_precision_sites() arguments "
        "[Note that this check verifies the shape of an alternative "
        "representation of Kmn. See the docs for the actual expected "
        "shape.]",
    )

    if L2 is None:
        L2 = L @ tf.linalg.matrix_transpose(L)

    m = Kuu.shape[-1]
    I = tf.eye(m, dtype=tf.float64)
    R = L2 + Kuu + I * jitter
    LR = tf.linalg.cholesky(R)
    LA = tf.linalg.cholesky(Kuu)[None]

    tmp1 = tf.linalg.triangular_solve(LR, Kuf)
    tmp2 = tf.linalg.triangular_solve(LA, Kuf)

    cov = Kff - tf.linalg.matrix_transpose(
        tf.reduce_sum(tf.square(tmp2), axis=-2) - tf.reduce_sum(tf.square(tmp1), axis=-2)
    )
    mean = tf.matmul(Kuf, tf.linalg.cholesky_solve(LR, l), transpose_a=True)[0]
    return mean, cov


def conditional_from_precision_sites(
    Kuu: tf.Tensor,
    Kff: tf.Tensor,
    Kuf: tf.Tensor,
    l: tf.Tensor,
    L: tf.Tensor = None,
    L2=None,
):
    """
    Given a gâ‚ and g2, and distribution p and q such that
      p(gâ‚‚) = N(gâ‚‚; 0, Kuu)

      p(gâ‚) = N(gâ‚; 0, Kff)
      p(gâ‚ | gâ‚‚) = N(gâ‚; Kfu (Kuuâ»Â¹) gâ‚‚, Kff - Kfu (Kuuâ»Â¹) Kuf)

    And  q(gâ‚‚) = N(gâ‚‚; Î¼, Î£) such that
        Î£â»Â¹  = Kuuâ»Â¹  + LLáµ€
        Î£â»Â¹Î¼ = l

    This method computes the mean and (co)variance of
      q(gâ‚) = âˆ« q(gâ‚‚) p(gâ‚ | gâ‚‚) dgâ‚‚ = N(gâ‚‚; Î¼*, Î£**)

    with
    Î£** = k** - kfu Kuuâ»Â¹ kuf - kfu Kuuâ»Â¹ Î£ Kuuâ»Â¹ kuf
        = k** - kfu Kuuâ»Â¹kuf + kfu Kuuâ»Â¹ (Kuuâ»Â¹  + LLáµ€)â»Â¹ Kuuâ»Â¹ kuf
        = k** - kfu Kuuâ»Â¹kuf + kfu Kuuâ»Â¹ (Kuu - Kuu L(I + Láµ€KuuL)â»Â¹Láµ€ Kuu) Kuuâ»Â¹ kuf
        = k** - kfu L(I + Láµ€KuuL)â»Â¹Láµ€ kuf
        = k** - kfu LWâ»Â¹Láµ€ kuf
        = k** - kfu L Lwâ»áµ€Lwâ»Â¹ Láµ€ kuf
        = k** - (Lwâ»Â¹ Láµ€ kuf)áµ€ Lwâ»Â¹ Láµ€ kuf
        = k** - (D kuf)áµ€ D kuf

    Î¼* = k*u Kuuâ»Â¹ m
       = k*u Kuuâ»Â¹ Î£ l
       = k*u Kuuâ»Â¹ (Kuuâ»Â¹  + LLáµ€)â»Â¹ l
       = k*u Kuuâ»Â¹ (Kuu - Kuu L(I + Láµ€KuuL)â»Â¹Láµ€ Kuu) l
       = k*u l - k*u L(I + Láµ€KuuL)â»Â¹Láµ€ Kuu l
       = k*u l - k*u LWâ»Â¹Láµ€ Kuu l
       = k*u l - (Lwâ»Â¹ Láµ€ kuf)áµ€ Lwâ»Â¹ Láµ€ Kuu l
       = k*u l - (D kuf)áµ€ D Kuu l

    Inputs:
    :param Kuu: tensor M x M
    :param Kff: tensor N x 1
    :param Kuf: tensor M x N
    :param L: tensor L x M x M
    :param l: tensor M x L
    """

    shape_constraints = [
        (Kuu, ["M", "M"]),
        (Kuf, ["M", "N"]),
        (Kff, ["N", 1]),
        (l, ["M", "L"]),
    ]

    if L2 is not None:
        shape_constraints.append(
            (L2, ["L", "M", "M"]),
        )
    if L is not None:
        shape_constraints.append(
            (L, ["L", "M", "M"]),
        )
    tf.debugging.assert_shapes(
        shape_constraints,
        message="conditional_from_precision_sites() arguments "
        "[Note that this check verifies the shape of an alternative "
        "representation of Kmn. See the docs for the actual expected "
        "shape.]",
    )

    if L is None:
        L = tf.linalg.cholesky(L2)

    m = Kuu.shape[-1]
    Id = tf.eye(m, dtype=default_float())
    C = tf.linalg.cholesky(Kuu)

    # W = I + Lâ‚œáµ€ Lâ‚š Lâ‚šáµ€ Lâ‚œ, chol(W)
    CtL = tf.matmul(C, L, transpose_a=True)
    W = Id + tf.matmul(CtL, CtL, transpose_a=True)
    chol_W = tf.linalg.cholesky(W)

    D = tf.linalg.triangular_solve(chol_W, tf.linalg.matrix_transpose(L))
    tmp = tf.matmul(D, Kuf)

    mean = tf.matmul(Kuf, l, transpose_a=True) - tf.linalg.matrix_transpose(
        tf.reduce_sum(
            tf.matmul(D, tf.matmul(Kuu, tf.linalg.matrix_transpose(l)[..., None])) * tmp, axis=-2
        )
    )

    cov = Kff - tf.linalg.matrix_transpose(tf.reduce_sum(tf.square(tmp), axis=-2))
    return mean, cov


def project_diag_sites(
    Kuf: tf.Tensor,
    lambda_1: tf.Tensor,
    lambda_2: tf.Tensor,
    Kuu: Optional[tf.Tensor] = None,
    cholesky=True,
):
    """
    From  Kuu, Kuf, Î»â‚, Î»â‚‚, computes statistics
    L = Kuuâ»Â¹ (Î£â‚™ Kufâ‚™ Î»â‚‚â‚™ Kfâ‚™u) Kuuâ»Â¹
    l = Kuuâ»Â¹ (Î£â‚™ Kufâ‚™ Î»â‚â‚™)

    :param Kuf: L x M x N
    :param lambda_2: N x L
    :param lambda_1: N x L
    :param Kuu: L x M x M

    return:
    l : M x L
    L : L x M x M
    """

    shape_constraints = [
        (Kuf, [..., "M", "N"]),
        (lambda_1, ["N", "L"]),
        (lambda_2, ["N", "L"]),
    ]
    if Kuu is not None:
        shape_constraints.append((Kuu, [..., "M", "M"]))
    tf.debugging.assert_shapes(
        shape_constraints,
        message="conditional_from_precision_sites() arguments "
        "[Note that this check verifies the shape of an alternative "
        "representation of Kmn. See the docs for the actual expected "
        "shape.]",
    )

    num_latent = lambda_1.shape[-1]
    P = tf.tile(Kuf[None], [num_latent, 1, 1]) if tf.rank(Kuf) == 2 else Kuf
    if Kuu is not None:
        Kuu = Kuu[None] if tf.rank(Kuu) == 2 else Kuu
        Luu = tf.linalg.cholesky(Kuu)
        P = tf.linalg.cholesky_solve(Luu, P)

    l = tf.einsum("lmn,nl->ml", P, lambda_1)
    L = tf.einsum("lmn,lon,nl->lmo", P, P, lambda_2)
    if cholesky:
        L = tf.linalg.cholesky(L)
    return l, L


def kl_from_precision_sites_white(A, l, L=None, L2=None):
    """
    Computes the kl divergence KL[q(f)|p(f)]

    where:
    q(f) = N(Î¼,Î£) and covariance of a Gaussian with natural parameters
    with
        Î£ = Î›â»Â¹  = (Aâ»Â¹  + Aâ»Â¹LLáµ€Aâ»Â¹)â»Â¹  = A(A + LLáµ€)â»Â¹A
        Î¼ = Î›â»Â¹ Aâ»Â¹l = Î£ Aâ»Â¹l

    p(f) = N(0,A)
    """

    shape_constraints = [
        (A, ["M", "M"]),
        (l, ["N", "L"]),
    ]
    if L is not None:
        shape_constraints.append((L, [..., "M", "M"]))
    if L2 is not None:
        shape_constraints.append((L2, [..., "M", "M"]))

    tf.debugging.assert_shapes(
        shape_constraints,
        message="kl_from_prec_precond() arguments "
        "[Note that this check verifies the shape of an alternative "
        "representation of Kmn. See the docs for the actual expected "
        "shape.]",
    )

    if L2 is None:
        L2 = L @ tf.linalg.matrix_transpose(L)
    m = tf.shape(L2)[-2]

    R = L2 + A
    LR = tf.linalg.cholesky(R)
    LA = tf.linalg.cholesky(A)

    # log det term
    log_det = tf.reduce_sum(tf.math.log(tf.square(tf.linalg.diag_part(LR)))) - tf.reduce_sum(
        tf.math.log(tf.square(tf.linalg.diag_part(LA)))
    )

    # trace term
    tmp = tf.linalg.triangular_solve(LR, LA)
    trace_plus_const = tf.reduce_sum(tf.square(tmp)) - tf.cast(m, A.dtype)

    # mahalanobis term
    mahalanobis = tf.reduce_sum(
        tf.square(tf.matmul(LA, tf.linalg.cholesky_solve(LR, l), transpose_a=True))
    )

    return 0.5 * (log_det + trace_plus_const + mahalanobis)


def kl_from_precision_sites(A, l, L=None, L2=None):
    """
    Computes the kl divergence KL[q(f)|p(f)]

    where:
    q(f) = N(Î¼,Î£) and covariance of a Gaussian with natural parameters
    with
        Î£ = Î›â»Â¹  = (Aâ»Â¹  + Aâ»Â¹LLáµ€Aâ»Â¹)â»Â¹  = A(A + LLáµ€)â»Â¹A
        Î¼ = Î›â»Â¹ Aâ»Â¹l = Î£ Aâ»Â¹l

    p(f) = N(0,A)
    """

    shape_constraints = [
        (A, ["M", "M"]),
        (l, ["N", "L"]),
    ]
    if L is not None:
        shape_constraints.append((L, [..., "M", "M"]))
    if L2 is not None:
        shape_constraints.append((L2, [..., "M", "M"]))

    tf.debugging.assert_shapes(
        shape_constraints,
        message="kl_from_prec_precond() arguments "
        "[Note that this check verifies the shape of an alternative "
        "representation of Kmn. See the docs for the actual expected "
        "shape.]",
    )

    if L2 is None:
        L2 = L @ tf.linalg.matrix_transpose(L)
    m = tf.shape(L2)[-2]

    R = L2 + A
    LR = tf.linalg.cholesky(R)
    LA = tf.linalg.cholesky(A)

    # log det term
    log_det = tf.reduce_sum(tf.math.log(tf.square(tf.linalg.diag_part(LR)))) - tf.reduce_sum(
        tf.math.log(tf.square(tf.linalg.diag_part(LA)))
    )

    # trace term
    tmp = tf.linalg.triangular_solve(LR, LA)
    trace_plus_const = tf.reduce_sum(tf.square(tmp)) - tf.cast(m, A.dtype)

    # mahalanobis term
    mahalanobis = tf.reduce_sum(
        tf.square(tf.matmul(LA, tf.linalg.cholesky_solve(LR, l), transpose_a=True))
    )

    return 0.5 * (log_det + trace_plus_const + mahalanobis)


def posterior_from_dense_site(K, lambda_1, lambda_2_sqrt):
    """
    Returns the mean and cholesky factor of the density q(u) = p(u)t(u) = ğ“(u; m, S)
    where p(u) = ğ“(u; 0, K) and t(u) = exp(uáµ€Î»â‚ - Â½ uáµ€Î›â‚‚u)

    P = -2Î›â‚‚

    S = (Kâ»Â¹ + P)â»Â¹ = (Kâ»Â¹ + LLáµ€)â»Â¹ = K - KLWâ»Â¹Láµ€K , W = (I + Láµ€KL)
    m = S Î»â‚

    Input:
    :param: K : M x M
    :param: lambda_1: M x P
    :param: lambda_2: P x M x M

    Output:
    m: M x P
    chol_S: P x M x M
    """
    shape_constraints = [
        (K, [..., "M", "M"]),
        (lambda_1, ["N", "L"]),
        (lambda_2_sqrt, ["L", "M", "M"]),
    ]
    tf.debugging.assert_shapes(
        shape_constraints,
        message="posterior_from_dense_site() arguments ",
    )

    L = lambda_2_sqrt  # L - likelihood precision square root
    m = K.shape[-1]
    Id = tf.eye(m, dtype=default_float())
    C = tf.linalg.cholesky(K)

    # W = I + Lâ‚œáµ€ Lâ‚š Lâ‚šáµ€ Lâ‚œ, chol(W)
    CtL = tf.matmul(C, L, transpose_a=True)
    W = Id + tf.matmul(CtL, CtL, transpose_a=True)
    chol_W = tf.linalg.cholesky(W)

    # S_q = K - K Wâ»Â¹K = K - [Lwâ»Â¹ K]áµ€[Lwâ»Â¹ K]
    LtK = tf.matmul(L, K, transpose_a=True)
    iwLtK = tf.linalg.triangular_solve(chol_W, LtK, lower=True, adjoint=False)
    S_q = K - tf.matmul(iwLtK, iwLtK, transpose_a=True)
    chol_S_q = tf.linalg.cholesky(S_q)
    m_q = tf.einsum("lmn,nl->ml", S_q, lambda_1)

    return m_q, chol_S_q


def posterior_from_dense_site_white(K, lambda_1, lambda_2, jitter=1e-9):
    """
    Returns the mean and cholesky factor of the density q(u) = p(u)t(u) = ğ“(u; m, S)
    where p(u) = ğ“(u; 0, K) and t(u) = exp(uáµ€Î»â‚ - Â½ uáµ€Î›â‚‚u)

    S = Î›â‚‚â»Â¹ = (Kâ»Â¹ + Kâ»Â¹PKâ»Â¹)â»Â¹ = = [Kâ»Â¹(K + P)Kâ»Â¹]â»Â¹  = K(K + P)â»Â¹K
    m = S Kâ»Â¹Î»â‚ = K(K + LLáµ€)â»Â¹Î»â‚

    Input:
    :param: K : M x M
    :param: lambda_1: M x P
    :param: lambda_2: P x M x M

    Output:
    m: M x P
    chol_S: P x M x M
    """

    shape_constraints = [(K, ["M", "M"]), (lambda_1, ["N", "L"]), (lambda_2, ["L", "M", "M"])]
    tf.debugging.assert_shapes(
        shape_constraints,
        message="posterior_from_dense_site_white() arguments ",
    )
    P = lambda_2  # L - likelihood precision square root
    m = K.shape[-1]
    Id = tf.eye(m, dtype=tf.float64)
    R = K + P
    LR = tf.linalg.cholesky(R + Id * jitter)
    iLRK = tf.linalg.triangular_solve(LR, K)
    S_q = tf.matmul(iLRK, iLRK, transpose_a=True)
    chol_S_q = tf.linalg.cholesky(S_q)
    m_q = (K @ tf.linalg.cholesky_solve(LR, lambda_1))[0]
    return m_q, chol_S_q


def gradient_transformation_mean_var_to_expectation(inputs, grads):
    """
    Transforms gradient ğ  of a function wrt [Î¼, ÏƒÂ²]
    into its gradients wrt to [Î¼, ÏƒÂ² + Î¼Â²]
    :param inputs: [Î¼, ÏƒÂ²]
    :param grads: ğ 
    Output:
    â–½Î¼
    """
    return grads[0] - 2.0 * tf.einsum("lmo,ol->ml", grads[1], inputs), grads[1]
